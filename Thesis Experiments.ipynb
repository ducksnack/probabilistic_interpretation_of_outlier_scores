{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f854d7",
   "metadata": {},
   "source": [
    "# THESIS EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f690bb",
   "metadata": {},
   "source": [
    "# Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3165c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "from scipy.stats import binom, chi2\n",
    "from scipy.io import arff\n",
    "\n",
    "from evaluate_ExCeeD_alt import *\n",
    "from ExCeeD_alt import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def create_data_point(dists):\n",
    "    \"\"\"\n",
    "    Create a data point with features given by input distribution parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    dists: A list of distributions given by lists or tuples that define the mean and standard deviation of each of\n",
    "           the random variables - e.g. [0,1] for a value drawn from a normal distribution with mean 0 and standard\n",
    "           deviation 1.\n",
    "           \n",
    "    Return: A list representing a data point on the form [x1, x2,...m xn] where n is the number of distributions defined\n",
    "            in the input dists\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data_point = []\n",
    "    for dist in dists:\n",
    "        x = np.random.normal(dist[0], dist[1])\n",
    "        data_point.append(x)\n",
    "    \n",
    "    return data_point\n",
    "        \n",
    "def create_data_set(dists, sample_size):\n",
    "    \"\"\"\n",
    "    Create a data set of size sample_size by using the create_data_point function.\n",
    "    \n",
    "    Parameters:\n",
    "    dists:         A list of distributions given by lists or tuples that define the mean and standard deviation of each of\n",
    "                   the random variables - e.g. [0,1] for a value drawn from a normal distribution with mean 0 and standard\n",
    "                   deviation 1.\n",
    "    sample_size:   The number of data points in the data set\n",
    "    \n",
    "    Returns: a list of data points. Data points defined as described in the create_data_point function\n",
    "    \"\"\"\n",
    "    data_points = []\n",
    "    for n in range(sample_size):\n",
    "        data_point = create_data_point(dists)\n",
    "        data_points.append(data_point)\n",
    "    \n",
    "    return data_points\n",
    "\n",
    "def compute_outlier_scores(df, outlier_method='lof'):\n",
    "    \"\"\"\n",
    "    Compute outlier scores for each point in a data frame using one of four outlier detection techniques.\n",
    "    \n",
    "    Parameters:\n",
    "    df:              DataFrame containing data to fit outlier detectio model to and then give outlier scores for.\n",
    "    outlier_method:  Which outlier detection method to use. The following are implemented:\n",
    "                         LocalOutlierFactor: to use LocalOutlierFactor use 'lof' as argument\n",
    "                         k-NearestNeighbors: to use k-NearestNeighbors use 'knn' as argument\n",
    "                         IsolationForest: to use IsolationForest use 'iforest' as argument\n",
    "                         One-Class Support Vector Machine: to use One-Class Support Vector Machine use 'ocsvm' as argument\n",
    "                         \n",
    "    Returns: List of outlier scores for all rows in df                     \n",
    "    \"\"\"\n",
    "    if outlier_method == 'lof':\n",
    "        lof = LocalOutlierFactor()\n",
    "        lof.fit_predict(df) \n",
    "        lof_scores = lof.negative_outlier_factor_   \n",
    "        lof_scores *= -1\n",
    "        outlier_scores = lof_scores\n",
    "    elif outlier_method == 'knn':\n",
    "        knn = KNN()\n",
    "        knn.fit(df)\n",
    "        knn_scores = knn.decision_scores_\n",
    "        outlier_scores = knn_scores\n",
    "    elif outlier_method == 'iforest':\n",
    "        iforest = IForest()\n",
    "        iforest.fit(df)\n",
    "        iforest_scores = iforest.decision_scores_\n",
    "        outlier_scores = iforest_scores\n",
    "    elif outlier_method == 'ocsvm':\n",
    "        ocsvm = OCSVM()\n",
    "        ocsvm.fit(df)\n",
    "        ocsvm_scores = ocsvm.decision_scores_\n",
    "        outlier_scores = ocsvm_scores\n",
    "    \n",
    "    return outlier_scores\n",
    "\n",
    "def cantelli_inequality(a, X):\n",
    "    \"\"\"\n",
    "    Use the Cantelli Inequality to compute lower outlier probability bound for a point a from the mean of distribution X\n",
    "    \n",
    "    Parameters:\n",
    "    a:    the distance from point, x, for which the lower outlier probability bound is to be computed and the mean of X\n",
    "    X:    the distribution from which x is drawn\n",
    "    \n",
    "    Returns: the lower outlier probability bound of x (as a float)\n",
    "    \"\"\"\n",
    "    var = np.var(X)\n",
    "    k = a/math.sqrt(var)\n",
    "    if k > 0:\n",
    "        p = min(1, (1/(1+k**2)))\n",
    "    else:\n",
    "        p = 1\n",
    "    return 1-p\n",
    "\n",
    "def compute_cantelli_probability_bounds(df, column):\n",
    "    \"\"\"\n",
    "    Use the cantelli_inequality function to compute lower outlier probability bounds for each row in column in\n",
    "    df based on the Cantelli Inequality\n",
    "    \n",
    "    Parameters:\n",
    "    df:     DataFrame from which the data is taken\n",
    "    column: String name of the column in df to use as distribution for calculation of probability bounds\n",
    "    \n",
    "    Returns: a list containing outlier probability bounds for each row in df\n",
    "    \"\"\"\n",
    "    props = []\n",
    "    dist_mean = df[column].mean()\n",
    "    for i,row in df.iterrows():\n",
    "        pr = cantelli_inequality(row[column]-dist_mean, df[column])\n",
    "        props.append(pr)\n",
    "    \n",
    "    return props\n",
    "\n",
    "def calculateMahalanobis(data_point, mu, cov):\n",
    "    \"\"\"\n",
    "    Calculate the mahalanobis distance from point data_point to the mean of the distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    data_point:    The data point for which to calculate mahalanobis distance\n",
    "    distribution:  The distribution from wich data_point is drawn\n",
    "    cov:           The covariance matrix of the distriution\n",
    "    \n",
    "    Returns: The mahalanobis distance from data_point to the mean of distriution\n",
    "    \"\"\"\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    diff_from_mean = data_point - mu\n",
    "    left = np.dot(diff_from_mean, inv_cov)\n",
    "    inner = np.dot(left, diff_from_mean)\n",
    "    d = math.sqrt(inner)\n",
    "    \n",
    "    return d\n",
    "\n",
    "def compute_mahalanobis_distance(df, cov=None):\n",
    "    \"\"\"\n",
    "    Use the calculateMahalanobis function to get the mahalanobis distance to the mean for all data points in df\n",
    "    \n",
    "    Parameters:\n",
    "    df:    DataFrame where each row represents a data point for which to compute the mahalanobis distance to the mean\n",
    "    cov:   Covariance matrix for the distribution from which x is drawn. If undefined, the function will define cov as\n",
    "    the covariance matrix for df.\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    if not cov:\n",
    "        cov = np.cov(df, rowvar=False)\n",
    "        \n",
    "    for i,row in df.iterrows():\n",
    "        d = calculateMahalanobis(row, df.mean(), cov)\n",
    "        dists.append(d)\n",
    "    \n",
    "    return dists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25931b6",
   "metadata": {},
   "source": [
    "# Section 2: How does the distribution of outlier probabilities look for the cantelli-approach?\n",
    "## Experiment 1\n",
    "\n",
    "### - Both for 2-dimensional data sets constructed for this purpose and the data sets in the ExCeeD experiments and for all four outlier detection methods\n",
    "\n",
    "This section contains experiments designed to see how the distribution of outlier probabilities look when using the Cantelli Inequality to evaluate outlier probabilities. This is done for several data sets and for four different outlier detection methods for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2-dimensinal dataset\n",
    "dists = [(0,1),(0,1)]\n",
    "data = create_data_set(dists, 500)\n",
    "df = pd.DataFrame(data, columns=['x1','x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26a18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute outlier scores and outlier probabilities (according to the Cantelli Inequality)\n",
    "df_copy = df.copy()\n",
    "df['lof_score'] = compute_outlier_scores(df_copy, 'lof')\n",
    "df['lof_prob (cant)'] = compute_cantelli_probability_bounds(df, 'lof_score')\n",
    "df['knn_score'] = compute_outlier_scores(df_copy, 'knn')\n",
    "df['knn_prob (cant)'] = compute_cantelli_probability_bounds(df, 'knn_score')\n",
    "df['iforest_score'] = compute_outlier_scores(df_copy, 'iforest')\n",
    "df['iforest_prob (cant)'] = compute_cantelli_probability_bounds(df, 'iforest_score')\n",
    "df['ocsvm_score'] = compute_outlier_scores(df_copy, 'ocsvm')\n",
    "df['ocsvm_prob (cant)'] = compute_cantelli_probability_bounds(df, 'ocsvm_score')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e9f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot histograms to visualize distribution of outlier scores\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(figsize=(10, 6), nrows=2, ncols=2)\n",
    "\n",
    "ax1.hist(df['lof_score'])\n",
    "ax1.set_title('LOF')\n",
    "ax1.set_xlabel('score')\n",
    "ax1.set_ylabel('count')\n",
    "\n",
    "ax2.hist(df['knn_score'])\n",
    "ax2.set_title('kNN')\n",
    "ax2.set_xlabel('score')\n",
    "ax2.set_ylabel('count')\n",
    "\n",
    "ax3.hist(df['iforest_score'])\n",
    "ax3.set_title('IForest')\n",
    "ax3.set_xlabel('score')\n",
    "ax3.set_ylabel('count')\n",
    "\n",
    "ax4.hist(df['ocsvm_score'])\n",
    "ax4.set_title('OCSVM')\n",
    "ax4.set_xlabel('score')\n",
    "ax4.set_ylabel('count')\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outlier probability for each outlier detection method\n",
    "fig, axes = plt.subplots(figsize=(10,6), nrows=2, ncols=2)\n",
    "\n",
    "axes[0,0].scatter(df['lof_score'], df['lof_prob (cant)'])\n",
    "axes[0,0].set_title('LOF')\n",
    "axes[0,0].set_xlabel('score')\n",
    "axes[0,0].set_ylabel('outlier probability')\n",
    "\n",
    "axes[0,1].scatter(df['knn_score'], df['knn_prob (cant)'])\n",
    "axes[0,1].set_title('kNN')\n",
    "axes[0,1].set_xlabel('score')\n",
    "axes[0,1].set_ylabel('outlier probability')\n",
    "\n",
    "axes[1,0].scatter(df['iforest_score'], df['iforest_prob (cant)'])\n",
    "axes[1,0].set_title('IForest')\n",
    "axes[1,0].set_xlabel('score')\n",
    "axes[1,0].set_ylabel('outlier probability')\n",
    "\n",
    "axes[1,1].scatter(df['ocsvm_score'], df['ocsvm_prob (cant)'])\n",
    "axes[1,1].set_title('OCSVM')\n",
    "axes[1,1].set_xlabel('score')\n",
    "axes[1,1].set_ylabel('outlier probability')\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment1-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find fraction of outlier scores that are above 0.80 or below 0.20 for the four outlier detection methods\n",
    "lof_frac = df[(df['lof_prob (cant)'] >= 0.8) | (df['lof_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "knn_frac = df[(df['knn_prob (cant)'] >= 0.8) | (df['knn_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "iforest_frac = df[(df['iforest_prob (cant)'] >= 0.8) | (df['iforest_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "ocsvm_frac = df[(df['ocsvm_prob (cant)'] >= 0.8) | (df['ocsvm_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "\n",
    "# collect fractions in DataFrame\n",
    "fracs = [[lof_frac, knn_frac, iforest_frac, ocsvm_frac]]\n",
    "columns = ['LOF', 'kNN', 'IForest', 'OCSVM']\n",
    "frac_df = pd.DataFrame(fracs, columns=columns)\n",
    "\n",
    "# display DataFrame\n",
    "frac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434e671",
   "metadata": {},
   "source": [
    "Repeat the above process with data set from \"benchmark datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set into DataFrame\n",
    "data = arff.loadarff('Benchmark_Datasets/WBC_norm_v02.arff') # WBC data set\n",
    "df = pd.DataFrame(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[:-2]].values #insert the effective features (no response label y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute outlier scores and Cantelli outlier probabilities for the different outlier detection methods\n",
    "df['lof_score'] = compute_outlier_scores(X, 'lof')\n",
    "df['lof_prob (cant)'] = compute_cantelli_probability_bounds(df, 'lof_score')\n",
    "df['knn_score'] = compute_outlier_scores(X, 'knn')\n",
    "df['knn_prob (cant)'] = compute_cantelli_probability_bounds(df, 'knn_score')\n",
    "df['iforest_score'] = compute_outlier_scores(X, 'iforest')\n",
    "df['iforest_prob (cant)'] = compute_cantelli_probability_bounds(df, 'iforest_score')\n",
    "df['ocsvm_score'] = compute_outlier_scores(X, 'ocsvm')\n",
    "df['ocsvm_prob (cant)'] = compute_cantelli_probability_bounds(df, 'ocsvm_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0f6c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot histograms to visualize distribution of outlier scores\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(figsize=(10, 6), nrows=2, ncols=2)\n",
    "\n",
    "ax1.hist(df['lof_score'])\n",
    "ax1.set_title('LOF')\n",
    "ax1.set_xlabel('score')\n",
    "ax1.set_ylabel('count')\n",
    "\n",
    "ax2.hist(df['knn_score'])\n",
    "ax2.set_title('kNN')\n",
    "ax2.set_xlabel('score')\n",
    "ax2.set_ylabel('count')\n",
    "\n",
    "ax3.hist(df['iforest_score'])\n",
    "ax3.set_title('IForest')\n",
    "ax3.set_xlabel('score')\n",
    "ax3.set_ylabel('count')\n",
    "\n",
    "ax4.hist(df['ocsvm_score'])\n",
    "ax4.set_title('OCSVM')\n",
    "ax4.set_xlabel('score')\n",
    "ax4.set_ylabel('count')\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment1-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff78354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize outlier probability for each outlier detection method\n",
    "fig, axes = plt.subplots(figsize=(10,6), nrows=2, ncols=2)\n",
    "\n",
    "axes[0,0].scatter(df['lof_score'], df['lof_prob (cant)'])\n",
    "axes[0,0].set_title('LOF')\n",
    "axes[0,0].set_xlabel('score')\n",
    "axes[0,0].set_ylabel('outlier probability')\n",
    "\n",
    "axes[0,1].scatter(df['knn_score'], df['knn_prob (cant)'])\n",
    "axes[0,1].set_title('kNN')\n",
    "axes[0,1].set_xlabel('score')\n",
    "axes[0,1].set_ylabel('outlier probability')\n",
    "\n",
    "axes[1,0].scatter(df['iforest_score'], df['iforest_prob (cant)'])\n",
    "axes[1,0].set_title('IForest')\n",
    "axes[1,0].set_xlabel('score')\n",
    "axes[1,0].set_ylabel('outlier probability')\n",
    "\n",
    "axes[1,1].scatter(df['ocsvm_score'], df['ocsvm_prob (cant)'])\n",
    "axes[1,1].set_title('OCSVM')\n",
    "axes[1,1].set_xlabel('score')\n",
    "axes[1,1].set_ylabel('outlier probability')\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment1-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffec420",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find fraction of outlier scores that are above 0.80 or below 0.20 for the four outlier detection methods\n",
    "frac_df_small = frac_df.copy()\n",
    "lof_frac = df[(df['lof_prob (cant)'] >= 0.8) | (df['lof_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "knn_frac = df[(df['knn_prob (cant)'] >= 0.8) | (df['knn_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "iforest_frac = df[(df['iforest_prob (cant)'] >= 0.8) | (df['iforest_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "ocsvm_frac = df[(df['ocsvm_prob (cant)'] >= 0.8) | (df['ocsvm_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "\n",
    "# collect fractions in DataFrame\n",
    "fracs = [lof_frac, knn_frac, iforest_frac, ocsvm_frac]\n",
    "frac_df_small.loc[frac_df_small.shape[0]] = fracs\n",
    "frac_df_small.rename(index={0:'Generated Data', 1:'Benchmark Dataset'}, inplace=True)\n",
    "\n",
    "# display DataFrame\n",
    "frac_df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots for all benchmark data sets\n",
    "indexes = ['Generated Data']\n",
    "datasets = os.listdir('Benchmark_Datasets/')[1:]\n",
    "for dataset in datasets:\n",
    "    data = arff.loadarff('Benchmark_Datasets/'+dataset)\n",
    "    df = pd.DataFrame(data[0])\n",
    "    X = df[df.columns[:-2]].values #insert the effective features (no response label y)\n",
    "    df['lof_score'] = compute_outlier_scores(X, 'lof')\n",
    "    df['lof_prob (cant)'] = compute_cantelli_probability_bounds(df, 'lof_score')\n",
    "    df['knn_score'] = compute_outlier_scores(X, 'knn')\n",
    "    df['knn_prob (cant)'] = compute_cantelli_probability_bounds(df, 'knn_score')\n",
    "    df['iforest_score'] = compute_outlier_scores(X, 'iforest')\n",
    "    df['iforest_prob (cant)'] = compute_cantelli_probability_bounds(df, 'iforest_score')\n",
    "    df['ocsvm_score'] = compute_outlier_scores(X, 'ocsvm')\n",
    "    df['ocsvm_prob (cant)'] = compute_cantelli_probability_bounds(df, 'ocsvm_score')\n",
    "    \n",
    "    fig, axes = plt.subplots(2,2)\n",
    "\n",
    "    # Visualize outlier probability for each outlier detection method\n",
    "    fig, axes = plt.subplots(2,2)\n",
    "\n",
    "    axes[0,0].scatter(df['lof_score'], df['lof_prob (cant)'])\n",
    "    axes[0,0].set_title('LOF')\n",
    "    axes[0,1].scatter(df['knn_score'], df['knn_prob (cant)'])\n",
    "    axes[0,1].set_title('kNN')\n",
    "    axes[1,0].scatter(df['iforest_score'], df['iforest_prob (cant)'])\n",
    "    axes[1,0].set_title('IForest')\n",
    "    axes[1,1].scatter(df['ocsvm_score'], df['ocsvm_prob (cant)'])\n",
    "    axes[1,1].set_title('OCSVM')\n",
    "    fig.suptitle(dataset[:-5], fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig('images/appendix/experiment1-'+dataset[:-5])\n",
    "    \n",
    "    lof_frac = df[(df['lof_prob (cant)'] >= 0.8) | (df['lof_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "    knn_frac = df[(df['knn_prob (cant)'] >= 0.8) | (df['knn_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "    iforest_frac = df[(df['iforest_prob (cant)'] >= 0.8) | (df['iforest_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "    ocsvm_frac = df[(df['ocsvm_prob (cant)'] >= 0.8) | (df['ocsvm_prob (cant)'] <= 0.2)].shape[0]/df.shape[0]\n",
    "\n",
    "    fracs = [lof_frac, knn_frac, iforest_frac, ocsvm_frac]\n",
    "    indexes.append(dataset[:-5])\n",
    "    \n",
    "    frac_df.loc[frac_df.shape[0]] = fracs\n",
    "\n",
    "frac_df.rename(index={i:name for i,name in enumerate(indexes)}, inplace=True)\n",
    "frac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d81d66",
   "metadata": {},
   "source": [
    "Results above look kinda as expected and based on these results the Cantelli Inequality seem like a reasonable method for evaluating outlier probabilities (it serves as noralization and behaves in a fairly reasonable way)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e10b4",
   "metadata": {},
   "source": [
    "# Section 3: Cantelli-ExCeeD\n",
    "## Experiment 2\n",
    "### - including the Cantelli-approach as outlier socre interpretation method and repeat the experiments from the Perini paper\n",
    "\n",
    "This section contains experiments designed to evaluate the Cantelli method against the ExCeeD method. This involves running an altered version of the evaluate_ExCeeD script from the Perini paper where the Cantelli method is included as an additional outlier probability interpretation method. This would then be the Cantelli-ExCeeD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b67929",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir('Benchmark_Datasets/')[2:]\n",
    "results = []\n",
    "i = len(datasets)\n",
    "t_start = time.time()\n",
    "for dataset in datasets:\n",
    "    i -=1\n",
    "    print(dataset[:-5])\n",
    "    data_path = 'Benchmark_Datasets/'+dataset\n",
    "    data = arff.loadarff(data_path)\n",
    "    df = pd.DataFrame(data[0])\n",
    "    \n",
    "    print(f'shape: {df.shape}')\n",
    "    \n",
    "    df['outlier'] = [string.decode(\"utf-8\") for string in df['outlier'].values]\n",
    "    y = np.asarray([1 if string == 'yes' else 0 for string in df['outlier'].values])\n",
    "    X = df[df.columns[:-2]].values #insert the effective features (no response label y)\n",
    "\n",
    "    model = 'KNN' #otherwise 'IForest', 'OCSVM'\n",
    "\n",
    "    L2_error = compute_confidence_error(model, X, y, saveresults = True) #dictionary with the L2 errors\n",
    "\n",
    "    results.append(L2_error)\n",
    "    \n",
    "    t_end = time.time()\n",
    "    print(f'finished in {t_end - t_start} seconds')\n",
    "    print(f'remaining datasets: {i}')\n",
    "    \n",
    "df_results_knn = pd.concat(results)\n",
    "dataset_names = [dataset[:-5] for dataset in datasets]\n",
    "df_results_knn.index = dataset_names\n",
    "df_results_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef940dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "i = len(datasets)\n",
    "t_start = time.time()\n",
    "for dataset in datasets:\n",
    "    i -=1\n",
    "    print(dataset[:-5])\n",
    "    data_path = 'Benchmark_Datasets/'+dataset\n",
    "    data = arff.loadarff(data_path)\n",
    "    df = pd.DataFrame(data[0])\n",
    "    \n",
    "    print(f'shape: {df.shape}')\n",
    "    \n",
    "    df['outlier'] = [string.decode(\"utf-8\") for string in df['outlier'].values]\n",
    "    y = np.asarray([1 if string == 'yes' else 0 for string in df['outlier'].values])\n",
    "    X = df[df.columns[:-2]].values #insert the effective features (no response label y)\n",
    "\n",
    "    model = 'IForest' #otherwise 'KNN', 'OCSVM'\n",
    "\n",
    "    L2_error = compute_confidence_error(model, X, y, saveresults = True) #dictionary with the L2 errors\n",
    "\n",
    "    results.append(L2_error)\n",
    "    \n",
    "    t_end = time.time()\n",
    "    print(f'finished in {t_end - t_start} seconds')\n",
    "    print(f'remaining datasets: {i}')\n",
    "    \n",
    "df_results_iforest = pd.concat(results)\n",
    "df_results_iforest.index = dataset_names\n",
    "df_results_iforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c4bd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "i = len(datasets)\n",
    "t_start = time.time()\n",
    "for dataset in datasets:\n",
    "    i -=1\n",
    "    print(dataset[:-5])\n",
    "    data_path = 'Benchmark_Datasets/'+dataset\n",
    "    data = arff.loadarff(data_path)\n",
    "    df = pd.DataFrame(data[0])\n",
    "    \n",
    "    print(f'shape: {df.shape}')\n",
    "    \n",
    "    df['outlier'] = [string.decode(\"utf-8\") for string in df['outlier'].values]\n",
    "    y = np.asarray([1 if string == 'yes' else 0 for string in df['outlier'].values])\n",
    "    X = df[df.columns[:-2]].values #insert the effective features (no response label y)\n",
    "\n",
    "    model = 'OCSVM' #otherwise 'KNN', 'IForest'\n",
    "\n",
    "    L2_error = compute_confidence_error(model, X, y, saveresults = True) #dictionary with the L2 errors\n",
    "\n",
    "    results.append(L2_error)\n",
    "    \n",
    "    t_end = time.time()\n",
    "    print(f'finished in {t_end - t_start} seconds')\n",
    "    print(f'remaining datasets: {i}')\n",
    "    \n",
    "df_results_ocsvm = pd.concat(results)\n",
    "df_results_ocsvm.index = dataset_names\n",
    "df_results_ocsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c374a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarize results - how often does the ExCeeD method beat each of the competitors?\n",
    "df_summary = pd.DataFrame([], columns=['ExCeeD wins', 'ExCeeD loses', 'ExCeeD draws'])\n",
    "\n",
    "competitors = df_results_ocsvm.columns[1:]\n",
    "\n",
    "for c in competitors:\n",
    "    \n",
    "    win_count = 0\n",
    "    draw_count = 0\n",
    "    \n",
    "    for df_results in [df_results_knn, df_results_iforest, df_results_ocsvm]:\n",
    "        wins = df_results[c] < df_results['ExCeeD']\n",
    "        draws = df_results[c] == df_results['ExCeeD']\n",
    "\n",
    "        if True in wins.value_counts():\n",
    "            win_count += wins.value_counts()[True]\n",
    "        else:\n",
    "            win_count += 0\n",
    "\n",
    "        if True in draws.value_counts():\n",
    "            draw_count += draws.value_counts()[True]\n",
    "        else:\n",
    "            draw_count += 0\n",
    "        \n",
    "\n",
    "\n",
    "    loss = 63 - (win_count + draw_count)\n",
    "    row = [loss, win_count, draw_count]\n",
    "    df_summary.loc[df_summary.shape[0]] = row\n",
    "\n",
    "df_summary.rename(index={i:name for i,name in enumerate(competitors)}, inplace=True)\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c789f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and standard deciation of rank (based on error in confidence) for each of the methods over all data sets\n",
    "ranks = [[],[],[],[],[],[],[],[],[],[]]\n",
    "for df_results in [df_results_knn, df_results_iforest, df_results_ocsvm]:\n",
    "    for i,row in df_results.iterrows():\n",
    "        for j in range(10):\n",
    "            ranks[j].append(row.rank()[j])\n",
    "    \n",
    "avg_ranks = []\n",
    "for r in ranks:\n",
    "    m = np.mean(r)\n",
    "    sd = np.std(r)\n",
    "    avg_ranks.append((m,sd))\n",
    "    \n",
    "avg_ranks\n",
    "\n",
    "df_ranks = pd.DataFrame(avg_ranks, columns=['mean', 'standard deviation'])\n",
    "df_ranks.rename(index={i+1:name for i,name in enumerate(competitors)}, inplace=True)\n",
    "df_ranks.rename(index={0:'ExCeeD'}, inplace=True)\n",
    "df_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c84691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mean and standard deciation of error in confidence for each of the methods over all data sets\n",
    "errors = [[],[],[],[],[],[],[],[],[],[]]\n",
    "for df_results in [df_results_knn, df_results_iforest, df_results_ocsvm]:\n",
    "    for i,row in df_results.iterrows():\n",
    "        for j in range(10):\n",
    "            errors[j].append(row[j])\n",
    "    \n",
    "avg_errors = []\n",
    "for e in errors:\n",
    "    m = np.mean(e)\n",
    "    sd = np.std(e)\n",
    "    avg_errors.append((m*100,sd*100))\n",
    "    \n",
    "avg_errors\n",
    "\n",
    "df_errors = pd.DataFrame(avg_errors, columns=['mean', 'standard deviation'])\n",
    "df_errors.rename(index={i+1:name for i,name in enumerate(competitors)}, inplace=True)\n",
    "df_errors.rename(index={0:'ExCeeD'}, inplace=True)\n",
    "df_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514673f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often does the Cantelli method beat each of the competitors?\n",
    "df_summary = pd.DataFrame([], columns=['ExCeeD_cant wins', 'ExCeeD_cant loses', 'ExCeeD_cant draws'])\n",
    "\n",
    "competitors = df_results_ocsvm.columns[2:]\n",
    "\n",
    "for c in competitors:\n",
    "    \n",
    "    win_count = 0\n",
    "    draw_count = 0\n",
    "    \n",
    "    for df_results in [df_results_knn, df_results_iforest, df_results_ocsvm]:\n",
    "        wins = df_results[c] < df_results['ExCeeD_cant']\n",
    "        draws = df_results[c] == df_results['ExCeeD_cant']\n",
    "\n",
    "        if True in wins.value_counts():\n",
    "            win_count += wins.value_counts()[True]\n",
    "        else:\n",
    "            win_count += 0\n",
    "\n",
    "        if True in draws.value_counts():\n",
    "            draw_count += draws.value_counts()[True]\n",
    "        else:\n",
    "            draw_count += 0\n",
    "        \n",
    "\n",
    "\n",
    "    loss = 63 - (win_count + draw_count)\n",
    "    row = [loss, win_count, draw_count]\n",
    "    df_summary.loc[df_summary.shape[0]] = row\n",
    "\n",
    "df_summary.rename(index={i:name for i,name in enumerate(competitors)}, inplace=True)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a7b10",
   "metadata": {},
   "source": [
    "# Section 4: Compare output of ExCeeD and Cantelli\n",
    "## Experiment 3\n",
    "\n",
    "This section contains some plots to compare the resulting outlier probabilities and confidences of the ExCeeD method and the Cantelli method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f560b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data set\n",
    "datasets = os.listdir('Benchmark_Datasets/')[1:] # drop first element in list (notebook checkpoint folder)\n",
    "dataset = datasets[-4] # Select data set - -4 is WBC\n",
    "data_path = 'Benchmark_Datasets/'+dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data set\n",
    "data = arff.loadarff(data_path) # Load selected data set\n",
    "df = pd.DataFrame(data[0])\n",
    "df['outlier'] = [string.decode(\"utf-8\") for string in df['outlier'].values]\n",
    "y = np.asarray([1 if string == 'yes' else 0 for string in df['outlier'].values])\n",
    "X = df[df.columns[:-2]].values #insert the effective features (no response label y)\n",
    "\n",
    "n = np.shape(X)[0]\n",
    "contamination = sum(y)/len(y)\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=331)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# fit kNN outlier detector and get scores    \n",
    "knno = KNN(n_neighbors=np.int(n*contamination), contamination = contamination).fit(X_train)\n",
    "train_scores_knno = knno.decision_function(X_train)\n",
    "test_scores_knno = knno.decision_function(X_test)\n",
    "prediction_knno = knno.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284591be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ExCeeD and Cantelli probabilities and confidences\n",
    "exceed_prob, exceed_conf = ExCeeD_alt(train_scores_knno, test_scores_knno, prediction_knno, contamination, alg='bayes')\n",
    "exceed_cant_prob, exceed_cant_conf = ExCeeD_alt(train_scores_knno, test_scores_knno, prediction_knno, contamination, alg='cant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame with score, probabilities, and confidences\n",
    "data = [test_scores_knno, exceed_prob, exceed_conf, exceed_cant_prob, exceed_cant_conf]\n",
    "df = pd.DataFrame(data).transpose()\n",
    "df.columns = ['Scores', 'ExCeeD_prob', 'ExCeeD_conf', 'Cant_prob', 'Cant_conf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probabilities and confidences\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(12, 5), nrows=1, ncols=2)\n",
    "ax1.scatter(x=df['Scores'], y=df['ExCeeD_prob'], label='ExCeeD')\n",
    "ax1.scatter(x=df['Scores'], y=df['Cant_prob'], label='Cantelli')\n",
    "ax1.set_title('Outlier Probability')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Outlier Probability')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "diff = [df['ExCeeD_conf'][i] - df['Cant_conf'][i] for i in range(df.shape[0])]\n",
    "ax2.scatter(x=df['Scores'], y=df['ExCeeD_conf'], label='ExCeeD', alpha=.6)\n",
    "ax2.scatter(x=df['Scores'], y=df['Cant_conf'], label='Cantelli', alpha=.6)\n",
    "ax2.scatter(x=df['Scores'], y=diff, label='Difference', alpha=.6)\n",
    "ax2.set_title('Confidence')\n",
    "ax2.set_xlabel('Score')\n",
    "ax2.set_ylabel('Confidence')\n",
    "ax2.legend(loc='center left')\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment3-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ce942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probabilities and confidences\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(12, 5), nrows=1, ncols=2)\n",
    "\n",
    "ax1.scatter(x=df['Scores'], y=df['ExCeeD_prob'], label='Probability', alpha=.5)\n",
    "ax1.scatter(x=df['Scores'], y=df['ExCeeD_conf'], label='Confidence', alpha=.5)\n",
    "ax1.set_title('ExCeeD')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Probability/Confidence')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "ax2.scatter(x=df['Scores'], y=df['Cant_prob'], label='Probability', alpha=.5)\n",
    "ax2.scatter(x=df['Scores'], y=df['Cant_conf'], label='Confidence', alpha=.5)\n",
    "ax2.set_title('Cantelli')\n",
    "ax2.set_xlabel('Score')\n",
    "ax2.set_ylabel('Probability/Confidence')\n",
    "ax2.legend(loc='center left')\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment3-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f890d",
   "metadata": {},
   "source": [
    "##### Test on multivariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3513fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bimodal bivariate distribution\n",
    "dists_test = [(0,1),(0,1)]\n",
    "dists2 = [(2,2), (2,2.2)]\n",
    "data_test = create_data_set(dists_test, 500)\n",
    "data2 = create_data_set(dists2, 30)\n",
    "data = data_test + data2\n",
    "y = np.array([0 for i in range(500)] + [1 for i in range(30)])\n",
    "data = np.array(data)\n",
    "df = pd.DataFrame(data, columns=['x1','x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81928cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=331)\n",
    "for train_index, test_index in sss.split(data, y):\n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "df_test = pd.DataFrame(X_test, columns=['x1','x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data set\n",
    "colors = np.array(['b' if i <= 500 else 'r' for i in range(df.shape[0])])\n",
    "sc = plt.scatter(x=df['x1'], y=df['x2'], c=colors)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.xlim([-5, 5])\n",
    "plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test set\n",
    "colors = ['b' if e == 0 else 'r' for e in y_test]\n",
    "sc = plt.scatter(x=df_test['x1'], y=df_test['x2'], c=colors)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.xlim([-5, 5])\n",
    "plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data set and test set\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(10,5), nrows=1, ncols=2)\n",
    "\n",
    "colors1 = np.array(['b' if i <= 500 else 'r' for i in range(df.shape[0])])\n",
    "colors2 = ['b' if e == 0 else 'r' for e in y_test]\n",
    "           \n",
    "ax1.scatter(x=df['x1'], y=df['x2'], c=colors1)\n",
    "ax1.set_title('Data Set')\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "xlim = ax1.get_xlim()\n",
    "ylim = ax1.get_ylim()\n",
    "\n",
    "ax2.scatter(x=df_test['x1'], y=df_test['x2'], c=colors2)\n",
    "ax2.set_title('Test Set')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "ax2.set_xlim(xlim)\n",
    "ax2.set_ylim(ylim)\n",
    "\n",
    "#fig.savefig('images/experiment3-dataset-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ba897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit kNN and get scores\n",
    "contamination = 30/530\n",
    "knno = KNN(n_neighbors=np.int(530*contamination), contamination = contamination).fit(X_train)\n",
    "train_scores_knno = knno.decision_function(X_train)\n",
    "test_scores_knno = knno.decision_function(X_test)\n",
    "prediction_knno = knno.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ExCeeD and Cantelli probabilities and confidences\n",
    "exceed_prob, exceed_conf = ExCeeD_alt(train_scores_knno, test_scores_knno, prediction_knno, contamination, alg='bayes')\n",
    "exceed_cant_prob, exceed_cant_conf = ExCeeD_alt(train_scores_knno, test_scores_knno, prediction_knno, contamination, alg='cant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918fa098",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceed_prob = np.array(exceed_prob)\n",
    "exceed_cant_prob = np.array(exceed_cant_prob)\n",
    "prob_diff = exceed_prob-exceed_cant_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probabilities and confidences\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(12, 5), nrows=1, ncols=2)\n",
    "ax1.scatter(x=test_scores_knno, y=exceed_prob, label='ExCeeD')\n",
    "ax1.scatter(x=test_scores_knno, y=exceed_cant_prob, label='Cantelli')\n",
    "ax1.set_title('Outlier Probability')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Outlier Probability')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "diff = [exceed_conf[i] - exceed_cant_conf[i] for i,v in enumerate(exceed_conf)]\n",
    "ax2.scatter(x=test_scores_knno, y=exceed_conf, label='ExCeeD', alpha=.6)\n",
    "ax2.scatter(x=test_scores_knno, y=exceed_cant_conf, label='Cantelli', alpha=.6)\n",
    "ax2.scatter(x=test_scores_knno, y=diff, label='Difference', alpha=.6)\n",
    "ax2.set_title('Confidence')\n",
    "ax2.set_xlabel('Score')\n",
    "ax2.set_ylabel('Confidence')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "#fig.savefig('images/experiment3-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8f189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize probabilities and confidences\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(12, 5), nrows=1, ncols=2)\n",
    "ax1.scatter(x=test_scores_knno, y=exceed_prob, label='Probability', alpha=.5)\n",
    "ax1.scatter(x=test_scores_knno, y=exceed_conf, label='Confidence', alpha=.5)\n",
    "ax1.set_title('ExCeeD')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Probability/Confidence')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "ax2.scatter(x=test_scores_knno, y=exceed_cant_prob, label='Probability', alpha=.5)\n",
    "ax2.scatter(x=test_scores_knno, y=exceed_cant_conf, label='Confidence', alpha=.5)\n",
    "ax2.set_title('Cantelli')\n",
    "ax2.set_xlabel('Score')\n",
    "ax2.set_ylabel('Probability/Confidence')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "#fig.savefig('images/experiment3-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6702565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probabilities and confidences\n",
    "fig, ((ax1, ax2),(ax3, ax4)) = plt.subplots(figsize=(12, 8), nrows=2, ncols=2)\n",
    "ax1.scatter(x=test_scores_knno, y=exceed_prob, label='ExCeeD')\n",
    "ax1.scatter(x=test_scores_knno, y=exceed_cant_prob, label='Cantelli')\n",
    "ax1.set_title('Outlier Probability')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Outlier Probability')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "diff = [exceed_conf[i] - exceed_cant_conf[i] for i,v in enumerate(exceed_conf)]\n",
    "ax2.scatter(x=test_scores_knno, y=exceed_conf, label='ExCeeD', alpha=.6)\n",
    "ax2.scatter(x=test_scores_knno, y=exceed_cant_conf, label='Cantelli', alpha=.6)\n",
    "ax2.scatter(x=test_scores_knno, y=diff, label='Difference', alpha=.6)\n",
    "ax2.set_title('Confidence')\n",
    "ax2.set_xlabel('Score')\n",
    "ax2.set_ylabel('Confidence')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "ax3.scatter(x=test_scores_knno, y=exceed_prob, label='Probability', alpha=.5)\n",
    "ax3.scatter(x=test_scores_knno, y=exceed_conf, label='Confidence', alpha=.5)\n",
    "ax3.set_title('ExCeeD')\n",
    "ax3.set_xlabel('Score')\n",
    "ax3.set_ylabel('Probability/Confidence')\n",
    "ax3.legend(loc='lower right')\n",
    "\n",
    "ax4.scatter(x=test_scores_knno, y=exceed_cant_prob, label='Probability', alpha=.5)\n",
    "ax4.scatter(x=test_scores_knno, y=exceed_cant_conf, label='Confidence', alpha=.5)\n",
    "ax4.set_title('Cantelli')\n",
    "ax4.set_xlabel('Score')\n",
    "ax4.set_ylabel('Probability/Confidence')\n",
    "ax4.legend(loc='lower right')\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment3-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b97a05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize differences in probabilities\n",
    "colors1 = exceed_prob\n",
    "colors2 = exceed_cant_prob\n",
    "colors3 = prob_diff\n",
    "\n",
    "colors = [colors1, colors2,colors3]\n",
    "\n",
    "p_min = min(np.concatenate(colors))\n",
    "p_max = max(np.concatenate(colors))\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(figsize=(10, 4), nrows=1, ncols=3)\n",
    "\n",
    "im1 = ax1.scatter(x=df_test['x1'], y=df_test['x2'], c=colors1, edgecolors='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "im2 = ax2.scatter(x=df_test['x1'], y=df_test['x2'], c=colors2, edgecolors='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "im3 = ax3.scatter(x=df_test['x1'], y=df_test['x2'], c=colors3, edgecolors='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "\n",
    "ax1.set_title('ExCeeD')\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax2.set_title('Cantelli')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "ax3.set_title('Difference')\n",
    "ax3.set_xlabel('x1')\n",
    "ax3.set_ylabel('x2')\n",
    "\n",
    "ax1.grid(linestyle = '--', linewidth = 0.5)\n",
    "ax2.grid(linestyle = '--', linewidth = 0.5)\n",
    "ax3.grid(linestyle = '--', linewidth = 0.5)\n",
    "\n",
    "\n",
    "cbar_ax = fig.add_axes([0.98, 0.2, 0.02, 0.65])\n",
    "fig.colorbar(im3, cax=cbar_ax)\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment3-4', bbox_inches='tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute true outlier probabilities\n",
    "real_prob = []\n",
    "dists = []\n",
    "mu = np.array([0,0])\n",
    "sigma = np.array([[1,0],[0,1]])\n",
    "for x in X_test:\n",
    "    m_dist_x = calculateMahalanobis(x, mu, sigma)\n",
    "    p = chi2.cdf(m_dist_x, df=2)\n",
    "    real_prob.append(p)\n",
    "\n",
    "real_prob = np.array(real_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341a182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize true outlier probabilities as well as differences between ExCeeD/Cantelli and true\n",
    "colors1 = real_prob\n",
    "colors2 = exceed_prob - real_prob\n",
    "colors3 = exceed_cant_prob - real_prob\n",
    "\n",
    "colors = [colors1, colors2,colors3]\n",
    "\n",
    "p_min = min(np.concatenate(colors))\n",
    "p_max = max(np.concatenate(colors))\n",
    "\n",
    "p_lim = max(abs(p_min), abs(p_max))\n",
    "\n",
    "p_min = -1*p_lim\n",
    "p_max = p_lim\n",
    "\n",
    "titles = ['True', 'ExCeeD','Cantelli']\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 4), nrows=1, ncols=3)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    im = ax.scatter(x=df_test['x1'], y=df_test['x2'], c=colors[i], edgecolors='black', cmap='seismic', vmin=p_min, vmax=p_max)\n",
    "    ax.grid(linestyle = '--', linewidth = 0.5)\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "\n",
    "\n",
    "cbar_ax = fig.add_axes([0.98, 0.2, 0.02, 0.65])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment3-5', bbox_inches='tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c37629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probabilities and confidences\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(12, 5), nrows=1, ncols=2)\n",
    "ax1.scatter(x=test_scores_knno, y=exceed_prob, label='ExCeeD')\n",
    "ax1.scatter(x=test_scores_knno, y=exceed_cant_prob, label='Cantelli')\n",
    "ax1.scatter(x=test_scores_knno, y=real_prob, label='True')\n",
    "ax1.set_title('Outlier Probability')\n",
    "ax1.set_xlabel('Outlier Score')\n",
    "ax1.set_ylabel('Outlier Probability')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "ax2.scatter(x=test_scores_knno, y=exceed_conf, label='ExCeeD', alpha=.5)\n",
    "ax2.scatter(x=test_scores_knno, y=exceed_cant_conf, label='Cantelli', alpha=.5)\n",
    "ax2.set_title('Confidence')\n",
    "ax2.set_xlabel('Score')\n",
    "ax2.set_ylabel('Confidence')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "#fig.savefig('images/experiment3-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb3eeb",
   "metadata": {},
   "source": [
    "# Section 5: Investigate the ExCeeD method more in depth\n",
    "## Experiment 4\n",
    "### - Is outlier \"rank\" actually what matters?\n",
    "### - How is the confidence expected to look?\n",
    "This section contains experiments made to evaluate the ExCeeD method and better understand it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011165db",
   "metadata": {},
   "source": [
    "### Experiment: Does ExCeeD evaluate scores or rankings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "dists1 = [(0,1),(0,1)]\n",
    "size_dataset = 500\n",
    "data_set1 = create_data_set(dists1, size_dataset)\n",
    "df = pd.DataFrame(data_set1, columns=['x1','x2'])\n",
    "df['dist'] = compute_mahalanobis_distance(df)\n",
    "df['outlier score1'] = [d/max(df['dist']) for d in df['dist']]\n",
    "df['outlier score2'] = [(d/max(df['dist']))**3 for d in df['dist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination = 0.02\n",
    "n_outliers = math.floor(contamination*size_dataset)\n",
    "t = df['outlier score1'].nlargest(n=n_outliers).iloc[-1]\n",
    "predictions = np.array([1 if s >= t else 0 for s in df['outlier score1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277250f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize outlier scores\n",
    "plt.scatter(df['dist'], df['outlier score1'])\n",
    "plt.scatter(df['dist'], df['outlier score2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14223d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the \"outlier scores\"\n",
    "fig, (ax1,ax2, ax3) = plt.subplots(figsize=(12, 4),nrows=1,ncols=3)\n",
    "\n",
    "color1 = df['outlier score1']\n",
    "color2 = df['outlier score2']\n",
    "colors = pd.concat([color1, color2])\n",
    "p_min = min(colors)\n",
    "p_max = max(colors)\n",
    "\n",
    "ax1.scatter(df['dist'], df['outlier score1'], label='Linear')\n",
    "ax1.scatter(df['dist'], df['outlier score2'], label='Cubed')\n",
    "ax1.set_title('Outlier Scores')\n",
    "ax1.set_xlabel('Mahalanobis Distance')\n",
    "ax1.set_ylabel('Outlier Score')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.scatter(df['x1'], df['x2'], c=color1, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax2.set_title('Linear')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "im = ax3.scatter(df['x1'], df['x2'], c=color2, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax3.set_title('Cubed')\n",
    "ax3.set_xlabel('x1')\n",
    "ax3.set_ylabel('x2')\n",
    "\n",
    "cbar_ax = fig.add_axes([0.98, 0.2, 0.02, 0.65])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment-setup-4-1', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ExCeeD and Cantelli probabilities and confidences\n",
    "exceed_prob1_full, exceed_conf1_full = ExCeeD_alt(df['outlier score1'], df['outlier score1'], predictions, contamination, alg='bayes')\n",
    "exceed_prob2_full, exceed_conf2_full = ExCeeD_alt(df['outlier score2'], df['outlier score2'], predictions, contamination, alg='bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44820d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors1 = exceed_prob1_full\n",
    "colors2 = exceed_prob2_full\n",
    "colors = [colors1, colors2]\n",
    "color_diff = np.array(colors1) - np.array(colors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aba4f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize outlier probabilities (and the difference)\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(figsize=(12, 4), nrows=1, ncols=3)\n",
    "p_min = min(np.concatenate(colors))\n",
    "p_max = max(np.concatenate(colors))\n",
    "ax1.scatter(df['x1'], df['x2'], c=colors1, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax1.set_title('Linear')\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax2.scatter(df['x1'], df['x2'], c=colors2, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax2.set_title('Cubed')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "im = ax3.scatter(df['x1'], df['x2'], c=color_diff, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax3.set_title('Difference')\n",
    "ax3.set_xlabel('x1')\n",
    "ax3.set_ylabel('x2')\n",
    "\n",
    "cbar_ax = fig.add_axes([0.98, 0.2, 0.02, 0.65])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment4-1', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bccc9f",
   "metadata": {},
   "source": [
    "There is no difference in estimated outlier probabilities for the points even though their outlier scores are guite different.\n",
    "\n",
    "Does it change if I split the data set in train and test data? Two tests:\n",
    "- 0.6/0.4 split\n",
    "- 0.3/0.7 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cc64c",
   "metadata": {},
   "source": [
    "###### First: the 0.6/0.4 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_scores_medium, test_scores_medium = train_test_split(df, test_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d22ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination = 0.02\n",
    "size_dataset = test_scores_medium.shape[0]\n",
    "n_outliers = math.floor(contamination*size_dataset)\n",
    "t = test_scores_medium['outlier score1'].nlargest(n=n_outliers).iloc[-1]\n",
    "predictions = np.array([1 if s >= t else 0 for s in test_scores_medium['outlier score1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08465f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ExCeeD and Cantelli probabilities and confidences\n",
    "exceed_prob1_medium, exceed_conf1_medium = ExCeeD_alt(train_scores_medium['outlier score1'], test_scores_medium['outlier score1'], predictions, contamination, alg='bayes')\n",
    "exceed_prob2_medium, exceed_conf2_medium = ExCeeD_alt(train_scores_medium['outlier score2'], test_scores_medium['outlier score2'], predictions, contamination, alg='bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors1 = exceed_prob1_medium\n",
    "colors2 = exceed_prob2_medium\n",
    "colors = [colors1, colors2]\n",
    "color_diff = np.array(colors1) - np.array(colors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a356bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize outlier probabilities (and the difference)\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(figsize=(12, 4), nrows=1, ncols=3)\n",
    "p_min = min(np.concatenate(colors))\n",
    "p_max = max(np.concatenate(colors))\n",
    "ax1.scatter(test_scores_medium['x1'], test_scores_medium['x2'], c=colors1, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax1.set_title('Linear')\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax2.scatter(test_scores_medium['x1'], test_scores_medium['x2'], c=colors2, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax2.set_title('Cubed')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "im = ax3.scatter(test_scores_medium['x1'], test_scores_medium['x2'], c=color_diff, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax3.set_title('Difference')\n",
    "ax3.set_xlabel('x1')\n",
    "ax3.set_ylabel('x2')\n",
    "\n",
    "cbar_ax = fig.add_axes([0.98, 0.2, 0.02, 0.65])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment4-2', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11573d",
   "metadata": {},
   "source": [
    "###### Second: the 0.3/0.7 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_scores, test_scores = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination = 0.02\n",
    "size_dataset = test_scores.shape[0]\n",
    "n_outliers = math.floor(contamination*size_dataset)\n",
    "t = test_scores['outlier score1'].nlargest(n=n_outliers).iloc[-1]\n",
    "predictions = np.array([1 if s >= t else 0 for s in test_scores['outlier score1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ExCeeD and Cantelli probabilities and confidences\n",
    "exceed_prob1_small, exceed_conf1_small = ExCeeD_alt(train_scores['outlier score1'], test_scores['outlier score1'], predictions, contamination, alg='bayes')\n",
    "exceed_prob2_small, exceed_conf2_small = ExCeeD_alt(train_scores['outlier score2'], test_scores['outlier score2'], predictions, contamination, alg='bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b429772",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors1 = exceed_prob1_small\n",
    "colors2 = exceed_prob2_small\n",
    "colors = [colors1, colors2]\n",
    "color_diff = np.array(colors1) - np.array(colors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb269e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize outlier probabilities (and the difference)\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(figsize=(12, 4), nrows=1, ncols=3)\n",
    "p_min = min(np.concatenate(colors))\n",
    "p_max = max(np.concatenate(colors))\n",
    "ax1.scatter(test_scores['x1'], test_scores['x2'], c=colors1, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax1.set_title('Linear')\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax2.scatter(test_scores['x1'], test_scores['x2'], c=colors2, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax2.set_title('Cubed')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "im = ax3.scatter(test_scores['x1'], test_scores['x2'], c=color_diff, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax3.set_title('Difference')\n",
    "ax3.set_xlabel('x1')\n",
    "ax3.set_ylabel('x2')\n",
    "\n",
    "cbar_ax = fig.add_axes([0.98, 0.2, 0.02, 0.65])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment4-3', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a838e84",
   "metadata": {},
   "source": [
    "No it did not!\n",
    "It would seem that the ExCeeD outlier probability estimation only evaluates based on outlier rank, not outlier score and it is not affected by splitting the data - in all three tests, there is no difference in outlier probability between the two fake outlier scores\n",
    "\n",
    "Is that also the case for the Cantelli?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e68d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cantelli outlier probabilities\n",
    "colors1 = compute_cantelli_probability_bounds(df, 'outlier score1')\n",
    "colors2 = compute_cantelli_probability_bounds(df, 'outlier score2')\n",
    "colors = [colors1, colors2]\n",
    "color_diff = np.array(colors1) - np.array(colors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6167ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outlier probabilities and difference\n",
    "p_min = min(np.concatenate(colors))\n",
    "p_max = max(np.concatenate(colors))\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(figsize=(12, 4), nrows=1, ncols=3)\n",
    "ax1.scatter(df['x1'], df['x2'], c=colors1, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax1.set_title('Linear')\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "ax2.scatter(df['x1'], df['x2'], c=colors2, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax2.set_title('Cubed')\n",
    "ax2.set_xlabel('x1')\n",
    "ax2.set_ylabel('x2')\n",
    "im = ax3.scatter(df['x1'], df['x2'], c=color_diff, edgecolor='black', cmap='hot_r', vmin=p_min, vmax=p_max)\n",
    "ax3.set_title('Difference')\n",
    "ax3.set_xlabel('x1')\n",
    "ax3.set_ylabel('x2')\n",
    "\n",
    "cbar_ax = fig.add_axes([0.98, 0.2, 0.02, 0.65])\n",
    "fig.colorbar(im, cax=cbar_ax)\n",
    "\n",
    "fig.tight_layout(pad=3)\n",
    "#fig.savefig('images/experiment4-4', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63615896",
   "metadata": {},
   "source": [
    "The cantelli approach of interpreting outlier probabilities based on oulier scores does not produce the same result for both 'outlier score1' and 'outlier score2' proving that it is in fact based on outlier score rather than outlier rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24673f42",
   "metadata": {},
   "source": [
    "So it seems that the ExCeeD method does not evaluate outlier scores, but rankings while the Cantelli method does evaluate scores. I need a discussion on the significance of this difference. Also maybe a discussion of why it works so well om the data sets.\n",
    "\n",
    "Do I need another/more experiments to illustrate that the ExCeeD evaluates based on ranking and not score? E.i. show that it is not coincidence for these specific \"scores\" I have chosen (linear and to cubed)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1796f550",
   "metadata": {},
   "source": [
    "### How is the confidence expected to look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup experiment parameters\n",
    "n = 200\n",
    "n_anom = 50\n",
    "X = np.linspace(0,1,100)\n",
    "p1 = X\n",
    "p2 = X**2\n",
    "dist_test1 = 1- binom.cdf(n - n_anom, n, p1)\n",
    "dist_test2 = 1- binom.cdf(n - n_anom, n, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4bc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(10,3), nrows=1, ncols=2)\n",
    "ax1.plot(X, dist_test1, label='Confidence, Y=1')\n",
    "ax1.plot(X, 1-dist_test1, label='Confidence, Y=0')\n",
    "ax1.plot(X, [dist_test1[i] if x > 0.7 else (1-dist_test1[i]) for i,x in enumerate(X)], alpha=0.5, linewidth=5, label='Expected Confidence')\n",
    "ax1.set_title('p = Relative Outlier Score')\n",
    "ax1.legend(loc='center left')\n",
    "ax1.set_xlabel('Relative Outlier Score')\n",
    "ax1.set_ylabel('Confidence')\n",
    "\n",
    "ax2.plot(X, dist_test2, label='Confidence, Y=1')\n",
    "ax2.plot(X, 1-dist_test2, label='Confidence, Y=0')\n",
    "ax2.plot(X, [dist_test2[i] if x > 0.7 else (1-dist_test2[i]) for i,x in enumerate(X)], alpha=0.55, linewidth=5, label='Expected Confidence')\n",
    "ax2.set_title('p = Relative Outlier Score^2')\n",
    "ax2.legend(loc='center left')\n",
    "ax2.set_xlabel('Relative Outlier Score')\n",
    "ax2.set_ylabel('Confidence')\n",
    "\n",
    "#fig.savefig('images/experiment4-5', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53801adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
